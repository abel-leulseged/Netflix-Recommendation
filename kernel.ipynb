{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "![](https://i0.wp.com/feminisminindia.com/wp-content/uploads/2018/07/sacred-games-season-2-netflix.jpg?fit=1280%2C720&ssl=1)"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": false
      },
      "cell_type": "markdown",
      "source": "---\n# Netflix Analytics - Movie Recommendation \n---"
    },
    {
      "metadata": {
        "_uuid": "0b2cddcc36b1fe69383693bb8c4bbb075214ab6c"
      },
      "cell_type": "markdown",
      "source": "---\n\n* The dataset I used here come directly from Netflix. It consists of 4 text data files, each file contains over 20M rows, i.e. over 4K movies and 400K customers. All together over 17K movies and 500K+ customers!\n\n* One of the major challenges is to get all these data loaded into the Kernel for analysis, I have encountered many times of Kernel running out of memory and tried many different ways of how to do it more efficiently. Welcome any suggestions!!!\n\n---"
    },
    {
      "metadata": {
        "_uuid": "1f51fce68da2749564319c17c8160e9ec3d891fc"
      },
      "cell_type": "markdown",
      "source": "---\n\n# **How To Recommend Anything?**\n\nTo support people best possible on their way through life, it is necessary to have an optimal recommendation on hand.\nWhether you want to introduce people among themselves in your social network, try to recommend a suitable supplement for the shopping basket of your customers or need a hint for yourself which movie to watch in the evening, there are unlimited possibilities to apply recommendation engines/systems around us.\n\n---"
    },
    {
      "metadata": {
        "_uuid": "df2d4c0e4aa5ba8fb404e822fccb14d8cf7b431f"
      },
      "cell_type": "markdown",
      "source": "1. [Import Libraries](#Import-Libraries)\n1. [Load Movie-Data](#Load-Data)\n1. [Load User-Data And Preprocess Data-Structure](#Userdata-and-Data-Stracture)\n1. [When Were The Movies Released?](#Movie-Release-Timeline)\n1. [How Are The Ratings Distributed?](#Rating-Distribution)\n1. [When Have The Movies Been Rated?](#Movie-has-been-rated)\n1. [How Are The Number Of Ratings Distributed For The Movies And The Users?](#Ratings-Distributed-MoviesvsUsers)\n1. [Filter Sparse Movies And Users](#Filter-Sparse-MovieUsers )\n1. [Create Train- And Testset](#Create-Train&Test)\n1. [Transform The User-Ratings To User-Movie-Matrix](#User-Ratings-To-User-Movie-Matrix)\n1. [Recommendation Engines](#Recommender-Engine)\n    1. [Mean Rating](#Mean-Rating)\n    1. [Weighted Mean Rating](#Weighted-Mean-Rating)\n    1. [Cosine User-User Similarity](#Cosine-User-User-Similarity)\n    1. [Cosine TFIDF Movie Description Similarity](#TFIDF-MovieDescription-Similarity)\n    1. [Matrix Factorisation With Keras And Gradient Descent](#Matrix-Factorisation-Keras-And-Gradient-Descent)\n    1. [Deep Learning With Keras](#Deep-Learning-With-Keras)\n    1. [Deep Hybrid System With Metadata And Keras](#Deep-Hybrid-System-With-Metadata-And-Keras)\n1. [Exploring Python Libraries](#Exploring-Python-Libraries)\n    1. [Surprise Library](#Surprise-Library)\n1. [Conclusion](#Conclusion)"
    },
    {
      "metadata": {
        "_uuid": "f8949ab1db8578ac7d492f01b0e784f1787d5669"
      },
      "cell_type": "markdown",
      "source": "## Import Libraries"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "036c8636d06aac7b12c61671112aa6441141986b"
      },
      "cell_type": "code",
      "source": "# To store the data\nimport pandas as pd\n\n# To do linear algebra\nimport numpy as np\n\n# To create plots\nimport matplotlib.pyplot as plt\n\n# To create interactive plots\nfrom plotly.offline import init_notebook_mode, plot, iplot\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\n\n# To shift lists\nfrom collections import deque\n\n# To compute similarities between vectors\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# To use recommender systems\nimport surprise as sp\nfrom surprise.model_selection import cross_validate\n\n# To create deep learning models\nfrom keras.layers import Input, Embedding, Reshape, Dot, Concatenate, Dense, Dropout\nfrom keras.models import Model\n\n# To create sparse matrices\nfrom scipy.sparse import coo_matrix\n\n# To light fm\nfrom lightfm import LightFM\nfrom lightfm.evaluation import precision_at_k\n\n# To stack sparse matrices\nfrom scipy.sparse import vstack\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "892c6c6e4ef6e7f0638dfec8d2b439c27a46300d"
      },
      "cell_type": "markdown",
      "source": "## Load Data "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8dc354230a8ffbba52f93b55975edb5ad3d18289"
      },
      "cell_type": "code",
      "source": "# Load data for all movies\nmovie_titles = pd.read_csv('../input/netflix-prize-data/movie_titles.csv', \n                           encoding = 'ISO-8859-1', \n                           header = None, \n                           names = ['Id', 'Year', 'Name']).set_index('Id')\n\nprint('Shape Movie-Titles:\\t{}'.format(movie_titles.shape))\nmovie_titles.sample(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1b9d560b1d911504963e0785c4dc5ca33841677a"
      },
      "cell_type": "markdown",
      "source": "---\nThere are roughly **18.000** movies in the ratings dataset and the metadata for the movies contains only the release date and the movie title.\n\n---"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "35ef2566256628c461bf8f08bfbd7bc9c4f4920b"
      },
      "cell_type": "code",
      "source": "# Load a movie metadata dataset\nmovie_metadata = pd.read_csv('../input/the-movies-dataset/movies_metadata.csv', low_memory=False)[['original_title', 'overview', 'vote_count']].set_index('original_title').dropna()\n# Remove the long tail of rarly rated moves\nmovie_metadata = movie_metadata[movie_metadata['vote_count']>10].drop('vote_count', axis=1)\n\nprint('Shape Movie-Metadata:\\t{}'.format(movie_metadata.shape))\nmovie_metadata.sample(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6d3178c905946971647375ac7805b1fc7148ef6a"
      },
      "cell_type": "markdown",
      "source": "---\nAbout **21.000** entries are in the movie metadata dataset.\n\n---"
    },
    {
      "metadata": {
        "_uuid": "2a4f51ade6fd7c9172f3e8fcb2ba67e67e88b055"
      },
      "cell_type": "markdown",
      "source": "## Userdata and Data Stracture\nThe user-data structure has to be preprocessed to extract all ratings and form a matrix, since the file-structure is a messy mixture of json and csv."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3c3e12a31f4ec138702e5fa0ea86f7395309ef3d"
      },
      "cell_type": "code",
      "source": "#The user-data structure has to be preprocessed to extract all ratings and form a matrix, since the file-structure is a messy mixture of json and csv.\n\n# Load single data-file\ndf_raw = pd.read_csv('../input/netflix-prize-data/combined_data_1.txt', header=None, names=['User', 'Rating', 'Date'], usecols=[0, 1, 2])\n\n\n# Find empty rows to slice dataframe for each movie\ntmp_movies = df_raw[df_raw['Rating'].isna()]['User'].reset_index()\nmovie_indices = [[index, int(movie[:-1])] for index, movie in tmp_movies.values]\n\n# Shift the movie_indices by one to get start and endpoints of all movies\nshifted_movie_indices = deque(movie_indices)\nshifted_movie_indices.rotate(-1)\n\n\n# Gather all dataframes\nuser_data = []\n\n# Iterate over all movies\nfor [df_id_1, movie_id], [df_id_2, next_movie_id] in zip(movie_indices, shifted_movie_indices):\n    \n    # Check if it is the last movie in the file\n    if df_id_1<df_id_2:\n        tmp_df = df_raw.loc[df_id_1+1:df_id_2-1].copy()\n    else:\n        tmp_df = df_raw.loc[df_id_1+1:].copy()\n        \n    # Create movie_id column\n    tmp_df['Movie'] = movie_id\n    \n    # Append dataframe to list\n    user_data.append(tmp_df)\n\n# Combine all dataframes\ndf = pd.concat(user_data)\ndel user_data, df_raw, tmp_movies, tmp_df, shifted_movie_indices, movie_indices, df_id_1, movie_id, df_id_2, next_movie_id\nprint('Shape User-Ratings:\\t{}'.format(df.shape))\ndf.sample(10)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f8cb7056604f7e435022167c689a2533e1e21e24"
      },
      "cell_type": "markdown",
      "source": "---\n\nThere are about ***24.000.000*** different ratings.\nI loaded only a single file of four to reduce memory footprint and accelerate computation. Keep in mind that this approach could introduce biases in the data.\n\n---"
    },
    {
      "metadata": {
        "_uuid": "ac25c2532c29e23d2855e4a7294b076c0696342e"
      },
      "cell_type": "markdown",
      "source": "## Movie Release Timeline"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ea5e1bdcfb83754dab86a12da1b8b4cc320563d6"
      },
      "cell_type": "code",
      "source": "# Get data\ndata = movie_titles['Year'].value_counts().sort_index()\n\n# Create trace\ntrace = go.Scatter(x = data.index,\n                   y = data.values,\n                    )\n# Create layout\nlayout = dict(title = '{} Movies Grouped By Year Of Release'.format(movie_titles.shape[0]),\n              xaxis = dict(title = 'Release Year'),\n              yaxis = dict(title = 'Movies'))\n\n# Create plot\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bcd3bc52959d07d16c748d58c66957fc8df501bf"
      },
      "cell_type": "markdown",
      "source": "---\nMany movies on Netflix have been released in this millennial. Whether Netflix prefers young movies or there are no old movies left can not be deduced from this plot.The decline for the rightmost point is probably caused by an incomplete last year.\n\n---"
    },
    {
      "metadata": {
        "_uuid": "4c0bd1e69ea36d80efda3cb496454fbef1c8547a"
      },
      "cell_type": "markdown",
      "source": "## Rating Distribution"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d8227be231769949d2ebc7f11ee11aaf7d459fe8"
      },
      "cell_type": "code",
      "source": "# Get data\ndata = df['Rating'].value_counts().sort_index(ascending=False)\n\n# Create trace\ntrace = go.Bar(x = data.index,\n               text = ['{:.1f} %'.format(val) for val in (data.values / df.shape[0] * 100)],\n               textposition = 'auto',\n               textfont = dict(color = '#000000'),\n               y = data.values,\n               )\n# Create layout\nlayout = dict(title = 'Distribution Of {} Netflix-Ratings'.format(df.shape[0]),\n              xaxis = dict(title = 'Rating'),\n              yaxis = dict(title = 'Count'))\n# Create plot\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "850e0ead10808d8ebf041e7aeb0fa487458bacb0"
      },
      "cell_type": "markdown",
      "source": "---\nNetflix movies rarely have a rating lower than three. Most ratings have between **three and four stars.**\nThe distribution is probably biased, since only people liking the movies proceed to be customers and others presumably will leave the platform.\n\n---"
    },
    {
      "metadata": {
        "_uuid": "2a14aa4854fd058a14b08866478fb9c96c849497"
      },
      "cell_type": "markdown",
      "source": "## Movie has been rated"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e51d6cb76101a15502f39eeddb7c653fa087e612"
      },
      "cell_type": "code",
      "source": "# Get data\ndata = df['Date'].value_counts()\ndata.index = pd.to_datetime(data.index)\ndata.sort_index(inplace=True)\n\n# Create trace\ntrace = go.Scatter(x = data.index,\n                   y = data.values,\n                  )\n# Create layout\nlayout = dict(title = '{} Movie-Ratings Grouped By Day'.format(df.shape[0]),\n              xaxis = dict(title = 'Date'),\n              yaxis = dict(title = 'Ratings'))\n\n# Create plot\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d0d54dd1fe9d81b4cb744acdd77313be860775cb"
      },
      "cell_type": "markdown",
      "source": "# Ratings Distributed MoviesvsUsers"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "16c5d3283d54b737efe32ea4788ebca860095415"
      },
      "cell_type": "code",
      "source": "# Get data\ndata = df.groupby('Movie')['Rating'].count().clip(upper=9999)\n\n# Create trace\ntrace = go.Histogram(x = data.values,\n                     name = 'Ratings',\n                     xbins = dict(start = 0,\n                                  end = 10000,\n                                  size = 100),\n                     marker = dict(color = '#db0000'))\n# Create layout\nlayout = go.Layout(title = 'Distribution Of Ratings Per Movie (Clipped at 9999)',\n                   xaxis = dict(title = 'Ratings Per Movie'),\n                   yaxis = dict(title = 'Count'),\n                   bargap = 0.2)\n\n# Create plot\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)\n\n\n\n##### Ratings Per User #####\n# Get data\ndata = df.groupby('User')['Rating'].count().clip(upper=199)\n\n# Create trace\ntrace = go.Histogram(x = data.values,\n                     name = 'Ratings',\n                     xbins = dict(start = 0,\n                                  end = 200,\n                                  size = 2),\n                     marker = dict(color = '#db0000'))\n# Create layout\nlayout = go.Layout(title = 'Distribution Of Ratings Per User (Clipped at 199)',\n                   xaxis = dict(title = 'Ratings Per User'),\n                   yaxis = dict(title = 'Count'),\n                   bargap = 0.2)\n\n# Create plot\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6a9722127be16657683963dcd0a39207c38fd699"
      },
      "cell_type": "markdown",
      "source": "The ratings per movie as well as the ratings per user both have nearly a perfect exponential decay. Only very few movies/users have many ratings.\n\n---\n"
    },
    {
      "metadata": {
        "_uuid": "db0a6e0a634d798509275e0613d7cf6bc6029255"
      },
      "cell_type": "markdown",
      "source": "# Filter Sparse MovieUsers "
    },
    {
      "metadata": {
        "_uuid": "5b8f82cb62b0077f962d43153c86fd1d5c61f9a3"
      },
      "cell_type": "markdown",
      "source": "To reduce the dimensionality of the dataset I am filtering rarely rated movies and rarely rating users out.\n\n---\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fe023d98fbd4327eb5bb88295c575c0012542521"
      },
      "cell_type": "code",
      "source": "# Filter sparse movies\nmin_movie_ratings = 10000\nfilter_movies = (df['Movie'].value_counts()>min_movie_ratings)\nfilter_movies = filter_movies[filter_movies].index.tolist()\n\n# Filter sparse users\nmin_user_ratings = 200\nfilter_users = (df['User'].value_counts()>min_user_ratings)\nfilter_users = filter_users[filter_users].index.tolist()\n\n# Actual filtering\ndf_filterd = df[(df['Movie'].isin(filter_movies)) & (df['User'].isin(filter_users))]\ndel filter_movies, filter_users, min_movie_ratings, min_user_ratings\nprint('Shape User-Ratings unfiltered:\\t{}'.format(df.shape))\nprint('Shape User-Ratings filtered:\\t{}'.format(df_filterd.shape))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5f2e44b3498fa98b7ae7f3c49bdaea5aadba1bf5"
      },
      "cell_type": "markdown",
      "source": "After filtering sparse movies and users about** 4.200.000** ratings are left.\n\n---"
    },
    {
      "metadata": {
        "_uuid": "75f37272926105cd1c76c3d0ef52b69d392ab433"
      },
      "cell_type": "markdown",
      "source": "# Create Train&Test"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5d4e095134c8f6336c32b89b4dab4f5f93f2ce78"
      },
      "cell_type": "code",
      "source": "# Shuffle DataFrame\ndf_filterd = df_filterd.drop('Date', axis=1).sample(frac=1).reset_index(drop=True)\n\n# Testingsize\nn = 100000\n\n# Split train- & testset\ndf_train = df_filterd[:-n]\ndf_test = df_filterd[-n:]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7b0693f9220cf176753e43fc44508c5a72cce263"
      },
      "cell_type": "markdown",
      "source": "The trainset will be used to train all models and the ***testset ensures comparibility*** between all models with the ***RMSE*** metric.\n\n---"
    },
    {
      "metadata": {
        "_uuid": "d4f3612bc4d291b6feeec0bef395f6d9b78e7d26"
      },
      "cell_type": "markdown",
      "source": "---\n# User-Ratings To User-Movie-Matrix\n\nA **large, sparse matrix** will be created in this step. Each row will represent a user and its ratings and the columns are the movies.\nThe interesting entries are the empty values in the matrix.\n\n**Empty values are unrated movies and could contain high values** and therefore should be good recommendations for the respective user.\n\nThe objective is to estimate the empty values to help our users.\n\n---"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d963c0e3bf02505fdff45cca07bdbf725a3ac225"
      },
      "cell_type": "code",
      "source": "# Create a user-movie matrix with empty values\ndf_p = df_train.pivot_table(index='User', columns='Movie', values='Rating')\nprint('Shape User-Movie-Matrix:\\t{}'.format(df_p.shape))\ndf_p.sample(3)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "43be425a64dbc8127434199abe4de0825fcd1952"
      },
      "cell_type": "markdown",
      "source": "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQn9TgAjTkbOSX5McNjk5EDmFyiuU1orzBEYvO-eOod34wS1l9V9Q)"
    },
    {
      "metadata": {
        "_uuid": "b17eb6fe9e06272d19a6d9f9787199ca1b59d823"
      },
      "cell_type": "markdown",
      "source": "# Recommender Engine"
    },
    {
      "metadata": {
        "_uuid": "1a610a00624f6145fc4e48e64aa2accb8d1074e7"
      },
      "cell_type": "markdown",
      "source": "---\n\n# Mean Rating\n\nComputing the **mean rating for all movies creates a ranking**. The recommendation will be the same for all users and can be used if there is no information on the user.\nVariations of this approach can be separate rankings for each country/year/gender/... and to use them individually to recommend movies/items to the user.\n\nIt has to be noted that this approach is **biased and favours movies with fewer ratings**, since large numbers of ratings tend to be less extreme in its mean ratings.\n\n---"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5423bc629c2a3a3dd4ba85cebcbd8b1d19071f95"
      },
      "cell_type": "code",
      "source": "# Top n movies\nn = 10\n\n# Compute mean rating for all movies\nratings_mean = df_p.mean(axis=0).sort_values(ascending=False).rename('Rating-Mean').to_frame()\n\n# Count ratings for all movies\nratings_count = df_p.count(axis=0).rename('Rating-Count').to_frame()\n\n# Combine ratings_mean, ratings_count and movie_titles\nranking_mean_rating = ratings_mean.head(n).join(ratings_count).join(movie_titles.drop('Year', axis=1))\n\n\n# Join labels and predictions\ndf_prediction = df_test.set_index('Movie').join(ratings_mean)[['Rating', 'Rating-Mean']]\ny_true = df_prediction['Rating']\ny_pred = df_prediction['Rating-Mean']\n\n# Compute RMSE\nrmse = np.sqrt(mean_squared_error(y_true=y_true, y_pred=y_pred))\n\n\n# Create trace\ntrace = go.Bar(x = ranking_mean_rating['Rating-Mean'],\n               text = ranking_mean_rating['Name'].astype(str) +': '+ ranking_mean_rating['Rating-Count'].astype(str) + ' Ratings',\n               textposition = 'outside',\n               textfont = dict(color = '#000000'),\n               orientation = 'h',\n               y = list(range(1, n+1)))\n# Create layout\nlayout = dict(title = 'Ranking Of Top {} Mean-Movie-Ratings: {:.4f} RMSE'.format(n, rmse),\n              xaxis = dict(title = 'Mean-Rating',\n                          range = (4.3, 4.55)),\n              yaxis = dict(title = 'Movie'))\n# Create plot\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ee5b009e135b3b623ae12ea6d35d12dc8ea452c8"
      },
      "cell_type": "markdown",
      "source": "---\n# Weighted Mean Rating\nTo tackle the problem of the unstable mean with few ratings **e.g. IDMb uses a weighted rating**. Many good ratings outweigh few in this algorithm.\n\n---"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "03c6ced6a912233903b68275e0623d224e0d94ce"
      },
      "cell_type": "code",
      "source": "# Number of minimum votes to be considered\nm = 1000\n\n# Mean rating for all movies\nC = df_p.stack().mean()\n\n# Mean rating for all movies separatly\nR = df_p.mean(axis=0).values\n\n# Rating count for all movies separatly\nv = df_p.count().values\n\n\n# Weighted formula to compute the weighted rating\nweighted_score = (v/ (v+m) *R) + (m/ (v+m) *C)\n# Sort ids to ranking\nweighted_ranking = np.argsort(weighted_score)[::-1]\n# Sort scores to ranking\nweighted_score = np.sort(weighted_score)[::-1]\n# Get movie ids\nweighted_movie_ids = df_p.columns[weighted_ranking]\n\n\n# Join labels and predictions\ndf_prediction = df_test.set_index('Movie').join(pd.DataFrame(weighted_score, index=weighted_movie_ids, columns=['Prediction']))[['Rating', 'Prediction']]\ny_true = df_prediction['Rating']\ny_pred = df_prediction['Prediction']\n\n# Compute RMSE\nrmse = np.sqrt(mean_squared_error(y_true=y_true, y_pred=y_pred))\n\n\n# Create DataFrame for plotting\ndf_plot = pd.DataFrame(weighted_score[:n], columns=['Rating'])\ndf_plot.index = weighted_movie_ids[:10]\nranking_weighted_rating = df_plot.join(ratings_count).join(movie_titles)\ndel df_plot\n\n\n# Create trace\ntrace = go.Bar(x = ranking_weighted_rating['Rating'],\n               text = ranking_weighted_rating['Name'].astype(str) +': '+ ranking_weighted_rating['Rating-Count'].astype(str) + ' Ratings',\n               textposition = 'outside',\n               textfont = dict(color = '#000000'),\n               orientation = 'h',\n               y = list(range(1, n+1)))\n# Create layout\nlayout = dict(title = 'Ranking Of Top {} Weighted-Movie-Ratings: {:.4f} RMSE'.format(n, rmse),\n              xaxis = dict(title = 'Weighted Rating',\n                          range = (4.15, 4.6)),\n              yaxis = dict(title = 'Movie'))\n# Create plot\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cc96d13a2a593c54262103abfb093d93306e9d55"
      },
      "cell_type": "markdown",
      "source": "The variable **\"m\" can be seen as regularizing parameter.** Changing it determines how much weight is put onto the movies with many ratings.\nEven if there is a better ranking the RMSE decreased slightly. There is a** trade-off between interpretability and predictive power.**"
    },
    {
      "metadata": {
        "_uuid": "5fa7fb88fc8c9e8d1d2ea0a4cd931cdc231d01ac"
      },
      "cell_type": "markdown",
      "source": "---\n\n# Cosine User-User Similarity"
    },
    {
      "metadata": {
        "_uuid": "02f2cd6ccc947cb3f02e8b79b8d554b919d3ae64"
      },
      "cell_type": "markdown",
      "source": "Interpreting each row of the matrix as a vector, a similarity between all user-vectors can be computed. This enables us to find all similar users and to work on user-specific recommendations. **Recommending high rated movies of similar users** to a specific user seems reasonable.\n\nSince there are still empty values left in the matrix, we have to use a reliable way to impute a decent value. A simple first approach is to **fill in the mean of each user into the empty values**\n\n\nAfterwards the** ratings of all similar users will be weighted with their similarity score and the mean will be computed**. Filtering for the unrated movies of a user reveals the best recommendations.\nYou can easily adapt this process to find similar items by computing the item-item similarity the same way. Since the matrix is mostly sparse and there are more users than items, this could be better for the RMSE score.\n\n---"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0a606f0a02aeda5941bdef9bb35725f949e84125"
      },
      "cell_type": "code",
      "source": "# User index for recommendation\nuser_index = 0\n\n# Number of similar users for recommendation\nn_recommendation = 100\n\n# Plot top n recommendations\nn_plot = 10\n\n\n# Fill in missing values\ndf_p_imputed = df_p.T.fillna(df_p.mean(axis=1)).T\n\n# Compute similarity between all users\nsimilarity = cosine_similarity(df_p_imputed.values)\n\n# Remove self-similarity from similarity-matrix\nsimilarity -= np.eye(similarity.shape[0])\n\n\n# Sort similar users by index\nsimilar_user_index = np.argsort(similarity[user_index])[::-1]\n# Sort similar users by score\nsimilar_user_score = np.sort(similarity[user_index])[::-1]\n\n\n# Get unrated movies\nunrated_movies = df_p.iloc[user_index][df_p.iloc[user_index].isna()].index\n\n# Weight ratings of the top n most similar users with their rating and compute the mean for each movie\nmean_movie_recommendations = (df_p_imputed.iloc[similar_user_index[:n_recommendation]].T * similar_user_score[:n_recommendation]).T.mean(axis=0)\n\n# Filter for unrated movies and sort results\nbest_movie_recommendations = mean_movie_recommendations[unrated_movies].sort_values(ascending=False).to_frame().join(movie_titles)\n\n\n# Create user-id mapping\nuser_id_mapping = {id:i for i, id in enumerate(df_p_imputed.index)}\n\nprediction = []\n# Iterate over all testset items\nfor user_id in df_test['User'].unique():\n    \n    # Sort similar users by index\n    similar_user_index = np.argsort(similarity[user_id_mapping[user_id]])[::-1]\n    # Sort similar users by score\n    similar_user_score = np.sort(similarity[user_id_mapping[user_id]])[::-1]\n    \n    for movie_id in df_test[df_test['User']==user_id]['Movie'].values:\n\n        # Compute predicted score\n        score = (df_p_imputed.iloc[similar_user_index[:n_recommendation]][movie_id] * similar_user_score[:n_recommendation]).values.sum() / similar_user_score[:n_recommendation].sum()\n        prediction.append([user_id, movie_id, score])\n        \n\n# Create prediction DataFrame\ndf_pred = pd.DataFrame(prediction, columns=['User', 'Movie', 'Prediction']).set_index(['User', 'Movie'])\ndf_pred = df_test.set_index(['User', 'Movie']).join(df_pred)\n\n\n# Get labels and predictions\ny_true = df_pred['Rating'].values\ny_pred = df_pred['Prediction'].values\n\n# Compute RMSE\nrmse = np.sqrt(mean_squared_error(y_true=y_true, y_pred=y_pred))\n\n\n# Create trace\ntrace = go.Bar(x = best_movie_recommendations.iloc[:n_plot, 0],\n               text = best_movie_recommendations['Name'],\n               textposition = 'inside',\n               textfont = dict(color = '#000000'),\n               orientation = 'h',\n               y = list(range(1, n_plot+1)),\n               marker = dict(color = '#db0000'))\n# Create layout\nlayout = dict(title = 'Ranking Of Top {} Recommended Movies For A User Based On Similarity: {:.4f} RMSE'.format(n_plot, rmse),\n              xaxis = dict(title = 'Recommendation-Rating',\n                           range = (4.1, 4.5)),\n              yaxis = dict(title = 'Movie'))\n# Create plot\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5ce49c038912fe442e31f92411ce4ce4d3fb8152"
      },
      "cell_type": "markdown",
      "source": "---\n# TFIDF MovieDescription Similarity\n\nIf there is no historical data for a user or there is reliable metadata for each movie, it can be useful to** compare the metadata of the movies to find similar ones.**\n\nIn this approch I will use the **movie description to create a TFIDF-matrix,** which counts and weights words in all descriptions, and compute a cosine similarity between all of those sparse text-vectors. This can easily be extended to more or different features if you like.\n\nUnfortunately it is impossible for this model to compute a **RMSE score**, since the model does not recommend the movies directly.\nIn this way it is possible to find movies closly related to each other, but it is **hard to find movies of different genres/categories**\n\n---"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "67363d6aa9a1abad1fa03b64897dea73d3748d55"
      },
      "cell_type": "code",
      "source": "# Create tf-idf matrix for text comparison\ntfidf = TfidfVectorizer(stop_words='english')\ntfidf_matrix = tfidf.fit_transform(movie_metadata['overview'].dropna())\n\n\n# Compute cosine similarity between all movie-descriptions\nsimilarity = cosine_similarity(tfidf_matrix)\n# Remove self-similarity from matrix\nsimilarity -= np.eye(similarity.shape[0])\n\n\n# Get index of movie to find similar movies\nmovie = 'Batman Begins'\nn_plot = 10\nindex = movie_metadata.reset_index(drop=True)[movie_metadata.index==movie].index[0]\n\n# Get indices and scores of similar movies\nsimilar_movies_index = np.argsort(similarity[index])[::-1][:n_plot]\nsimilar_movies_score = np.sort(similarity[index])[::-1][:n_plot]\n\n# Get titles of similar movies\nsimilar_movie_titles = movie_metadata.iloc[similar_movies_index].index\n\n\n# Create trace\ntrace = go.Bar(x = similar_movies_score,\n               text = similar_movie_titles,\n               textposition = 'inside',\n               textfont = dict(color = '#000000'),\n               orientation = 'h',\n               y = list(range(1, n_plot+1)),\n               marker = dict(color = '#db0000'))\n# Create layout\nlayout = dict(title = 'Ranking Of Top {} Most Similar Movie Descriptions For \"{}\"'.format(n_plot, movie),\n              xaxis = dict(title = 'Cosine TFIDF Description Similarity',\n                           range = (0, 0.4)),\n              yaxis = dict(title = 'Movie'))\n# Create plot\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0e368475d3b5414f64549242a8da679cacbf9bdd"
      },
      "cell_type": "markdown",
      "source": "---\n#  Matrix Factorisation-Keras And Gradient Descent\n\nThe **user-movie rating matrix is high dimensional and sparse**, therefore I am going to reduce the dimensionality to represent the data in a dense form.\n\n**Using matrix factorisation a large matrix can be estimated/decomposed into two long but slim matrices**. With gradient descent it is possible to adjust these matrices to represent the given ratings. The **gradient descent algorithm finds latent variables which represent the underlying structure of the dataset.** Afterwards these latent variables can be used to reconstruct the original matrix and to predict the missing ratings for each user.\n\nIn this case the model has not been trained to convergence and is not hyperparameter optimized.\n\n---"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4b52a3d547fbcf76a904fbc2164b7301a1d04184"
      },
      "cell_type": "code",
      "source": "# Create user- & movie-id mapping\nuser_id_mapping = {id:i for i, id in enumerate(df_filterd['User'].unique())}\nmovie_id_mapping = {id:i for i, id in enumerate(df_filterd['Movie'].unique())}\n\n\n# Create correctly mapped train- & testset\ntrain_user_data = df_train['User'].map(user_id_mapping)\ntrain_movie_data = df_train['Movie'].map(movie_id_mapping)\n\ntest_user_data = df_test['User'].map(user_id_mapping)\ntest_movie_data = df_test['Movie'].map(movie_id_mapping)\n\n\n# Get input variable-sizes\nusers = len(user_id_mapping)\nmovies = len(movie_id_mapping)\nembedding_size = 10\n\n\n##### Create model\n# Set input layers\nuser_id_input = Input(shape=[1], name='user')\nmovie_id_input = Input(shape=[1], name='movie')\n\n# Create embedding layers for users and movies\nuser_embedding = Embedding(output_dim=embedding_size, \n                           input_dim=users,\n                           input_length=1, \n                           name='user_embedding')(user_id_input)\nmovie_embedding = Embedding(output_dim=embedding_size, \n                            input_dim=movies,\n                            input_length=1, \n                            name='item_embedding')(movie_id_input)\n\n# Reshape the embedding layers\nuser_vector = Reshape([embedding_size])(user_embedding)\nmovie_vector = Reshape([embedding_size])(movie_embedding)\n\n# Compute dot-product of reshaped embedding layers as prediction\ny = Dot(1, normalize=False)([user_vector, movie_vector])\n\n# Setup model\nmodel = Model(inputs=[user_id_input, movie_id_input], outputs=y)\nmodel.compile(loss='mse', optimizer='adam')\n\n\n# Fit model\nmodel.fit([train_user_data, train_movie_data],\n          df_train['Rating'],\n          batch_size=256, \n          epochs=1,\n          validation_split=0.1,\n          shuffle=True)\n\n# Test model\ny_pred = model.predict([test_user_data, test_movie_data])\ny_true = df_test['Rating'].values\n\n#  Compute RMSE\nrmse = np.sqrt(mean_squared_error(y_pred=y_pred, y_true=y_true))\nprint('\\n\\nTesting Result With Keras Matrix-Factorization: {:.4f} RMSE'.format(rmse))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5378eff41cf27da52afeb965a0add5e0915e1eb7"
      },
      "cell_type": "markdown",
      "source": "---\n# Deep Learning With Keras"
    },
    {
      "metadata": {
        "_uuid": "2279f069fc842d80e42bedded7c178e2207acc6a"
      },
      "cell_type": "markdown",
      "source": "With its embedding layers this is similar to the matrix factorization approach above, but instead of using a fixed dot-product as recommendation we will utilize some **dense layers** so the network can find better combinations.\n\n---"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "78fa00b97ab836b72e8835a2b4c737c196f1a8cd"
      },
      "cell_type": "code",
      "source": "# Setup variables\nuser_embedding_size = 20\nmovie_embedding_size = 10\n\n\n##### Create model\n# Set input layers\nuser_id_input = Input(shape=[1], name='user')\nmovie_id_input = Input(shape=[1], name='movie')\n\n# Create embedding layers for users and movies\nuser_embedding = Embedding(output_dim=user_embedding_size, \n                           input_dim=users,\n                           input_length=1, \n                           name='user_embedding')(user_id_input)\nmovie_embedding = Embedding(output_dim=movie_embedding_size, \n                            input_dim=movies,\n                            input_length=1, \n                            name='item_embedding')(movie_id_input)\n\n# Reshape the embedding layers\nuser_vector = Reshape([user_embedding_size])(user_embedding)\nmovie_vector = Reshape([movie_embedding_size])(movie_embedding)\n\n# Concatenate the reshaped embedding layers\nconcat = Concatenate()([user_vector, movie_vector])\n\n# Combine with dense layers\ndense = Dense(256)(concat)\ny = Dense(1)(dense)\n\n# Setup model\nmodel = Model(inputs=[user_id_input, movie_id_input], outputs=y)\nmodel.compile(loss='mse', optimizer='adam')\n\n\n# Fit model\nmodel.fit([train_user_data, train_movie_data],\n          df_train['Rating'],\n          batch_size=256, \n          epochs=1,\n          validation_split=0.1,\n          shuffle=True)\n\n# Test model\ny_pred = model.predict([test_user_data, test_movie_data])\ny_true = df_test['Rating'].values\n\n#  Compute RMSE\nrmse = np.sqrt(mean_squared_error(y_pred=y_pred, y_true=y_true))\nprint('\\n\\nTesting Result With Keras Deep Learning: {:.4f} RMSE'.format(rmse))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bbe959aa13093d4df5e1121e420d817630cdbe63"
      },
      "cell_type": "markdown",
      "source": "---\n# Deep Hybrid System With Metadata And Keras\nOne advantage of deep learning models is, that movie-metadata can easily be added to the model.\nI will **tf-idf transform** the short description of all movies to a sparse vector. The model will learn to reduce the dimensionality of this vector and how to combine metadata with the embedding of the **user-id and the movie-id**. In this way you can add any additional metadata to your own recommender.\nThese kind of hybrid systems can learn how to reduce the impact of the cold start problem.\n\n---"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e613267ec8e71203950c898d6f24f28a232d1c24",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Create user- & movie-id mapping\nuser_id_mapping = {id:i for i, id in enumerate(df['User'].unique())}\nmovie_id_mapping = {id:i for i, id in enumerate(df['Movie'].unique())}\n\n# Use mapping to get better ids\ndf['User'] = df['User'].map(user_id_mapping)\ndf['Movie'] = df['Movie'].map(movie_id_mapping)\n\n\n##### Combine both datasets to get movies with metadata\n# Preprocess metadata\ntmp_metadata = movie_metadata.copy()\ntmp_metadata.index = tmp_metadata.index.str.lower()\n\n# Preprocess titles\ntmp_titles = movie_titles.drop('Year', axis=1).copy()\ntmp_titles = tmp_titles.reset_index().set_index('Name')\ntmp_titles.index = tmp_titles.index.str.lower()\n\n# Combine titles and metadata\ndf_id_descriptions = tmp_titles.join(tmp_metadata).dropna().set_index('Id')\ndf_id_descriptions['overview'] = df_id_descriptions['overview'].str.lower()\ndel tmp_metadata,tmp_titles\n\n\n# Filter all ratings with metadata\ndf_hybrid = df.drop('Date', axis=1).set_index('Movie').join(df_id_descriptions).dropna().drop('overview', axis=1).reset_index().rename({'index':'Movie'}, axis=1)\n\n\n# Split train- & testset\nn = 100000\ndf_hybrid = df_hybrid.sample(frac=1).reset_index(drop=True)\ndf_hybrid_train = df_hybrid[:1500000]\ndf_hybrid_test = df_hybrid[-n:]\n\n\n# Create tf-idf matrix for text comparison\ntfidf = TfidfVectorizer(stop_words='english')\ntfidf_hybrid = tfidf.fit_transform(df_id_descriptions['overview'])\n\n\n# Get mapping from movie-ids to indices in tfidf-matrix\nmapping = {id:i for i, id in enumerate(df_id_descriptions.index)}\n\ntrain_tfidf = []\n# Iterate over all movie-ids and save the tfidf-vector\nfor id in df_hybrid_train['Movie'].values:\n    index = mapping[id]\n    train_tfidf.append(tfidf_hybrid[index])\n    \ntest_tfidf = []\n# Iterate over all movie-ids and save the tfidf-vector\nfor id in df_hybrid_test['Movie'].values:\n    index = mapping[id]\n    test_tfidf.append(tfidf_hybrid[index])\n\n\n# Stack the sparse matrices\ntrain_tfidf = vstack(train_tfidf)\ntest_tfidf = vstack(test_tfidf)\n\n\n##### Setup the network\n# Network variables\nuser_embed = 10\nmovie_embed = 10\n\n\n# Create two input layers\nuser_id_input = Input(shape=[1], name='user')\nmovie_id_input = Input(shape=[1], name='movie')\ntfidf_input = Input(shape=[24144], name='tfidf', sparse=True)\n\n# Create separate embeddings for users and movies\nuser_embedding = Embedding(output_dim=user_embed,\n                           input_dim=len(user_id_mapping),\n                           input_length=1,\n                           name='user_embedding')(user_id_input)\nmovie_embedding = Embedding(output_dim=movie_embed,\n                            input_dim=len(movie_id_mapping),\n                            input_length=1,\n                            name='movie_embedding')(movie_id_input)\n\n# Dimensionality reduction with Dense layers\ntfidf_vectors = Dense(128, activation='relu')(tfidf_input)\ntfidf_vectors = Dense(32, activation='relu')(tfidf_vectors)\n\n# Reshape both embedding layers\nuser_vectors = Reshape([user_embed])(user_embedding)\nmovie_vectors = Reshape([movie_embed])(movie_embedding)\n\n# Concatenate all layers into one vector\nboth = Concatenate()([user_vectors, movie_vectors, tfidf_vectors])\n\n# Add dense layers for combinations and scalar output\ndense = Dense(512, activation='relu')(both)\ndense = Dropout(0.2)(dense)\noutput = Dense(1)(dense)\n\n\n# Create and compile model\nmodel = Model(inputs=[user_id_input, movie_id_input, tfidf_input], outputs=output)\nmodel.compile(loss='mse', optimizer='adam')\n\n\n# Train and test the network\nmodel.fit([df_hybrid_train['User'], df_hybrid_train['Movie'], train_tfidf],\n          df_hybrid_train['Rating'],\n          batch_size=1024, \n          epochs=2,\n          validation_split=0.1,\n          shuffle=True)\n\ny_pred = model.predict([df_hybrid_test['User'], df_hybrid_test['Movie'], test_tfidf])\ny_true = df_hybrid_test['Rating'].values\n\nrmse = np.sqrt(mean_squared_error(y_pred=y_pred, y_true=y_true))\nprint('\\n\\nTesting Result With Keras Hybrid Deep Learning: {:.4f} RMSE'.format(rmse))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e289420296d52ceb5afdd44e891a14390fe61064"
      },
      "cell_type": "markdown",
      "source": "# Exploring Python Libraries\n"
    },
    {
      "metadata": {
        "_uuid": "005cf599d0155f82844c61299144c1b2d9fe3ee1"
      },
      "cell_type": "markdown",
      "source": "# Surprise Library\n\nThe surprise library was built for creating and analyzing recommender systems.\nIt has to be mentioned that most of the built-in algorithms use some kind of the above approches. I am going to compare these algorithms to each other in this section using 3-fold crossvalidation. Since the algorithms and the dataset have a large memoryfootprint the comparison will be executed on a subsampled dataset which is not comparable to the above models"
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "_uuid": "3fb0a33876b29dad35c797585d2a9e017b0dd9da"
      },
      "cell_type": "code",
      "source": "# Load dataset into surprise specific data-structure\ndata = sp.Dataset.load_from_df(df_filterd[['User', 'Movie', 'Rating']].sample(20000), sp.Reader())\n\nbenchmark = []\n# Iterate over all algorithms\nfor algorithm in [sp.SVD(), sp.SVDpp(), sp.SlopeOne(), sp.NMF(), sp.NormalPredictor(), sp.KNNBaseline(), sp.KNNBasic(), sp.KNNWithMeans(), sp.KNNWithZScore(), sp.BaselineOnly(), sp.CoClustering()]:\n    # Perform cross validation\n    results = cross_validate(algorithm, data, measures=['RMSE', 'MAE'], cv=3, verbose=False)\n    \n    # Get results & append algorithm name\n    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n    tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n    \n    # Store data\n    benchmark.append(tmp)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "_uuid": "48fd5fbea9a82b61389e1c4ff57bba46f30da636"
      },
      "cell_type": "code",
      "source": "# Store results\nsurprise_results = pd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse', ascending=False)\n\n# Get data\ndata = surprise_results[['test_rmse', 'test_mae']]\ngrid = data.values\n\n# Create axis labels\nx_axis = [label.split('_')[1].upper() for label in data.columns.tolist()]\ny_axis = data.index.tolist()\n\nx_label = 'Function'\ny_label = 'Algorithm'\n\n\n# Get annotations and hovertext\nhovertexts = []\nannotations = []\nfor i, y_value in enumerate(y_axis):\n    row = []\n    for j, x_value in enumerate(x_axis):\n        annotation = grid[i, j]\n        row.append('Error: {:.3f}<br>{}: {}<br>{}: {}<br>Fit Time: {:.3f}s<br>Test Time: {:.3f}s'.format(annotation, y_label, y_value ,x_label, x_value, surprise_results.loc[y_value]['fit_time'], surprise_results.loc[y_value]['test_time']))\n        annotations.append(dict(x=x_value, y=y_value, text='{:.3f}'.format(annotation), ax=0, ay=0, font=dict(color='#000000')))\n    hovertexts.append(row)\n\n# Create trace\ntrace = go.Heatmap(x = x_axis,\n                   y = y_axis,\n                   z = data.values,\n                   text = hovertexts,\n                   hoverinfo = 'text',\n                   colorscale = 'Picnic',\n                   colorbar = dict(title = 'Error'))\n\n# Create layout\nlayout = go.Layout(title = 'Crossvalidated Comparison Of Surprise Algorithms',\n                   xaxis = dict(title = x_label),\n                   yaxis = dict(title = y_label,\n                                tickangle = -40),\n                   annotations = annotations)\n\n# Create plot\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d9bec3af0735f92aa77adc8208bde015631f2983"
      },
      "cell_type": "markdown",
      "source": "# Conclusion\nThere are many different ways to set up a recommender system and just like other machine learning algorithms it is very important to know which objective has to be optimized and therefore which layout should be choosen."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "da3e207bcbbe62a8adb9bd6d4b8fb9d9f9026de9"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}